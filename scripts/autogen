#!/usr/bin/env python3
# Copyright (c) The mlkem-native project authors
# Copyright (c) The mldsa-native project authors
# SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT

import subprocess
import platform
import argparse
import pathlib
import re
import pyparsing as pp
import sys
import os

from concurrent.futures import ThreadPoolExecutor
from functools import partial

modulus = 8380417
root_of_unity = 1753
montgomery_factor = pow(2, 32, modulus)

# This file re-generated auto-generated source files in mldsa-native.
#
# It currently covers:
# - zeta values for the reference NTT and invNTT
# - header guards


def status_update(task, msg):
    print(f"\r{'':<140}", end="", flush=True)
    print(f"\r[{task}]: {msg} ...", end="", flush=True)


def gen_header():
    yield "/*"
    yield " * Copyright (c) The mldsa-native project authors"
    yield " * SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT"
    yield " */"
    yield ""
    yield "/*"
    yield " * WARNING: This file is auto-generated from scripts/autogen"
    yield " *          Do not modify it directly."
    yield " */"
    yield ""


def get_files(pattern):
    return [str(p) for p in pathlib.Path().glob(pattern) if p.is_file()]


def get_c_source_files():
    return get_files("mldsa/**/*.c")


def get_asm_source_files():
    return get_files("mlkem/**/*.S")


def get_header_files():
    return get_files("mldsa/**/*.h")


def format_content(content):
    p = subprocess.run(
        ["clang-format"], capture_output=True, input=content, text=True, shell=True
    )
    if p.returncode != 0:
        print(p.stderr)
        print(
            f"Failed to auto-format autogenerated code (clang-format return code {p.returncode}). Are you running in a nix shell? See BUILDING.md."
        )
        exit(1)
    return p.stdout


class CondParser:
    """Rudimentary parser for expressions if `#if .. #else ..` directives"""

    def __init__(self):
        c_identifier = pp.common.identifier()
        c_integer_suffix = pp.one_of("U L LU UL LL ULL LLU", caseless=True)
        c_dec_integer = pp.Combine(
            pp.Optional(pp.one_of("+ -"))
            + pp.Word(pp.nums)
            + pp.Optional(c_integer_suffix)
        )
        c_hex_integer = pp.Combine(
            pp.Literal("0x") + pp.Word(pp.hexnums) + pp.Optional(c_integer_suffix)
        )

        self.parser = pp.infix_notation(
            c_identifier | c_hex_integer | c_dec_integer,
            [
                (pp.one_of("!"), 1, pp.opAssoc.RIGHT),
                (pp.one_of("!= == <= >= > <"), 2, pp.opAssoc.LEFT),
                (pp.one_of("&&"), 2, pp.opAssoc.LEFT),
                (pp.one_of("||"), 2, pp.opAssoc.LEFT),
            ],
        )

    @staticmethod
    def connective(res):
        """Extract the top-level connective for the expression"""
        if not isinstance(res, list):
            return None
        elif len(res) == 2:
            # Unary operator (will be "!" in our case)
            return res[0]
        else:
            # Binary operator
            return res[1]

    @staticmethod
    def map_top(f, res):
        """Apply function to arguments of top-level connective"""
        if not isinstance(res, list):
            return res
        else:
            # We expect `f` to do nothing on strings, so it is safe
            # to apply it everywhere, including the connectives.
            return list(map(f, res))

    @staticmethod
    def args(res):
        """Assuming the argument is a binary operation, return all arguments"""
        return res[::2]

    @staticmethod
    def simplify_double_negation(res):
        """Cancel double negations"""
        if CondParser.connective(res) == "!" and CondParser.connective(res[1]) == "!":
            res = res[1][1]
        res = CondParser.map_top(CondParser.simplify_double_negation, res)
        return res

    @staticmethod
    def simplify_not_eq(res):
        """Replace !(x == y) by x != y, and !(x != y) by x == y"""
        if CondParser.connective(res) == "!" and CondParser.connective(res[1]) == "==":
            res = res[1]
            res[1] = "!="
        if CondParser.connective(res) == "!" and CondParser.connective(res[1]) == "!=":
            res = res[1]
            res[1] = "=="
        res = CondParser.map_top(CondParser.simplify_not_eq, res)
        return res

    @staticmethod
    def simplify_neq_chain(res):
        """Check for &&-chains of inequalities followed by an equality
        which implies the inequality. This catches patterns like
        ```
         #if MLKEM_K == 2
         ...
         #elif MLKEM_K == 3
         ...
         #elif MLKEM_K == 4
         ...
         #endif
        ```
        """
        if (
            CondParser.connective(res) == "&&"
            and CondParser.connective(res[-1]) == "=="
        ):
            lhs = res[-1][0]
            rhs = res[-1][2]
            args = []
            for a in CondParser.args(res[:-1]):
                if CondParser.connective(a) == "!=" and a[0] == lhs:
                    args.append(a[2])
                else:
                    args = None
                    break
            if args is None:
                return
            # Check if all args are numerical and different
            if rhs.isdigit() and all(
                map(lambda a: a.isdigit() and int(a) != int(rhs), args)
            ):
                # Success -- just drop all but the final condition
                return res[-1]
        res = CondParser.map_top(CondParser.simplify_neq_chain, res)
        return res

    @staticmethod
    def print_exp(exp, inner=False):
        conn = CondParser.connective(exp)
        if conn is None:
            return exp
        elif conn == "!":
            res = f"!{CondParser.print_exp(exp[1], inner=True)}"
        else:
            padded_conn = f" {conn} "
            res = padded_conn.join(
                map(lambda e: CondParser.print_exp(e, inner=True), CondParser.args(exp))
            )
        if inner is True and conn in ["&&", "||"]:
            res = f"({res})"
        return res

    def simplify_assoc(exp):
        """Check for unnecesary bracketing and remove it"""
        conn = CondParser.connective(exp)
        if conn in ["&&", "||"]:
            args = CondParser.args(exp)
            new_args = []
            for a in args:
                if CondParser.connective(a) == conn:
                    new_args += CondParser.args(a)
                else:
                    new_args.append(a)
            exp = [x for y in map(lambda x: [x, conn], new_args) for x in y][:-1]
        exp = CondParser.map_top(CondParser.simplify_assoc, exp)
        return exp

    def simplify_all(exp):
        exp = CondParser.simplify_double_negation(exp)
        exp = CondParser.simplify_not_eq(exp)
        exp = CondParser.simplify_neq_chain(exp)
        exp = CondParser.simplify_assoc(exp)
        return exp

    def parse_condition(self, filename, exp, simplify=True):
        try:
            exp = self.parser.parseString(exp, parseAll=True).as_list()[0]
        except pp.ParseException:
            print(
                f"\nWARNING: Ignoring condition '{exp}' from {filename} I cannot parse"
            )
            return exp
        if simplify is True:
            exp = CondParser.simplify_all(exp)
        return exp

    def normalize_condition(self, filename, exp):
        return CondParser.print_exp(self.parse_condition(filename, exp))


def adjust_preprocessor_comments_for_filename(content, source_file):
    """Automatically add comments to large `#if ... #else ... #endif`
    blocks indicating the guarding conditions.

    For example, a block

    ```c
      #if FOO
      ...
      #else
      ...
      #endif
    ```

    will be transformed into


    ```c
      #if FOO
      ...
      #else /* FOO */
      ...
      #endif /* !FOO */
    ```

    except when the distance between the preprocessor directives is
    very short, and the annotations would be more harmful than useful.

    ```
    """
    status_update("if-else", source_file)

    content = content.split("\n")
    new_content = []

    # Stack of `#if` statements. Every entry is a tuple
    # `(conds, line_no, if_or_else, has_children)`, where
    # - `conds` is the list of conditions being tested.
    #   In a normal `#if ... #else ...` braach, this is a singleton list
    #   containing the condition being tested. In a chain of
    #   `#if .. #elif ..` it contains all conditions encountered to this point.
    # - `line_no` is the line where it started
    # - `if_or_else` indicates whether we are in the `#if`
    #   or the `#else` branch (if present)
    # - `force_print` indicates if a comment should be omitted
    if_stack = []

    def merge_escaped_lines(l, i):
        while l.endswith("\\"):
            l = l.removesuffix("\\").rstrip() + content[i + 1].lstrip()
            i = i + 1
        return (l, i)

    def merge_commented_lines(l, i):
        # Not very robust, but good enough
        if not "/*" in l or "*/" in l:
            return (l, i)
        i += 1
        while "*/" not in content[i]:
            l += content[i]
            i += 1

        l += content[i]
        return (l, i)

    def should_print(cur_line_no, conds, line_no, force_print):
        line_threshold = 5
        if force_print is True:
            return True

        if cur_line_no - line_no >= line_threshold:
            return True
        return False

    parser = CondParser()

    def format_condition(cond):
        cond = re.sub(r"defined\(([^)]+)\)", r"\1", cond)
        return parser.normalize_condition(source_file, cond)

    def format_conditions(conds, branch):
        prev_conds = list(map(lambda s: f"!({s})", conds[:-1]))
        final_cond = conds[-1]
        if branch is False:
            final_cond = f"!({final_cond})"
        full_cond = "&&".join(prev_conds + [final_cond])
        return format_condition(full_cond)

    def adhoc_format(line):
        # .c and .h files are formatted as a whole
        if not source_file.endswith(".S"):
            return line
        return format_content(line)

    i = 0
    while i < len(content):
        l = content[i].strip()
        # Replace #ifdef by #if defined(...)
        if l.startswith("#ifdef "):
            l = "#if defined(" + l.removeprefix("#ifdef").strip() + ")"
        if l.startswith("#ifndef "):
            l = "#if !defined(" + l.removeprefix("#ifndef").strip() + ")"
        if l.startswith("#if"):
            l, _ = merge_escaped_lines(l, i)
            cond = l.removeprefix("#if")
            if_stack.append(([cond], i, True, False))
            new_content.append(content[i])
        elif l.startswith("#elif"):
            conds, _, _, force_print = if_stack.pop()
            l, _ = merge_escaped_lines(l, i)
            conds.append(l.removeprefix("#elif"))
            if_stack.append((conds, i, True, force_print))
            new_content.append(content[i])
        elif l.startswith("#else"):
            l, i = merge_escaped_lines(l, i)
            _, i = merge_commented_lines(l, i)
            conds, j, branch, force_print = if_stack.pop()
            assert branch is True
            print_else = should_print(i, cond, j, force_print)
            if_stack.append((conds, i, False, print_else))
            if print_else is True:
                cond = format_conditions(conds, True)
                new_content.append(adhoc_format("#else /* " + cond + " */"))
            else:
                new_content.append("#else")
        elif l.startswith("#endif"):
            l, i = merge_escaped_lines(l, i)
            _, i = merge_commented_lines(l, i)
            conds, j, branch, force_print = if_stack.pop()
            print_endif = should_print(i, conds, j, force_print)
            if print_endif is False:
                new_content.append("#endif")
            else:
                cond = format_conditions(conds, branch)
                new_content.append(adhoc_format("#endif /* " + cond + " */"))
        else:
            # Skip over multiline comments -- we don't want to
            # handle `#if ...` inside documentation as this would
            # lead to nested `/* ... */`.
            i_old = i
            _, i = merge_commented_lines(l, i_old)
            new_content += content[i_old : i + 1]
        i += 1

    return "\n".join(new_content)


def gen_preprocessor_comments_for(source_file, dry_run=False):
    with open(source_file, "r") as f:
        content = f.read()
    new_content = adjust_preprocessor_comments_for_filename(content, source_file)
    update_file(source_file, new_content, dry_run=dry_run)


def gen_preprocessor_comments(dry_run=False):
    files = get_c_source_files() + get_asm_source_files() + get_header_files()
    with ThreadPoolExecutor() as executor:
        results = list(
            executor.map(partial(gen_preprocessor_comments_for, dry_run=dry_run), files)
        )


def update_file(filename, content, dry_run=False, force_format=False):

    if force_format is True or filename.endswith((".c", ".h", ".i")):
        content = adjust_preprocessor_comments_for_filename(content, filename)
        content = format_content(content)

    if dry_run is False:
        with open(filename, "w+") as f:
            f.write(content)
    else:
        if os.path.exists(filename) is False:
            print(f"Autogenerated file {filename} does not exist")
            exit(1)
        with open(filename, "r") as f:
            current_content = f.read()
        if current_content != content:
            filename_new = f"{filename}.new"
            print(
                f"Autogenerated file {filename} needs updating. Have you called scripts/autogen?",
                file=sys.stderr,
            )
            print(f"Writing new version to {filename_new}", file=sys.stderr)
            with open(filename_new, "w") as f:
                f.write(content)
            subprocess.run(["diff", filename, filename_new])
            exit(1)


def bitreverse(i, n):
    r = 0
    for _ in range(n):
        r = 2 * r + (i & 1)
        i >>= 1
    return r


def signed_reduce(a):
    """Return signed canonical representative of a mod b"""
    c = a % modulus
    if c >= modulus / 2:
        c -= modulus
    return c


def gen_c_zetas():
    """Generate source and header file for zeta values used in
    the reference NTT and invNTT"""

    # The zeta values are the powers of the chosen root of unity (17),
    # converted to Montgomery form.

    zeta = [0]  # First entry is unused and set to 0
    for i in range(1, 256):
        zeta.append(signed_reduce(pow(root_of_unity, i, modulus) * montgomery_factor))

    # The source code stores the zeta table in bit reversed form
    yield from (zeta[bitreverse(i, 8)] for i in range(256))


def gen_c_zeta_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield "#include <stdint.h>"
        yield ""
        yield "/*"
        yield " * Table of zeta values used in the reference NTT and inverse NTT."
        yield " * See autogen for details."
        yield " */"
        yield "static const int32_t mld_zetas[MLDSA_N] = {"
        yield from map(lambda t: str(t) + ",", gen_c_zetas())
        yield "};"
        yield ""

    update_file("mldsa/zetas.inc", "\n".join(gen()), dry_run=dry_run, force_format=True)


def prepare_root_for_barrett(root):
    """Takes a constant that the code needs to Barrett-multiply with,
    and returns the pair of (a) its signed canonical form, (b) the
    twisted constant used in the high-mul part of the Barrett multiplication."""

    # Signed canonical reduction
    root = signed_reduce(root)

    def round_to_even(t):
        rt = round(t)
        if rt % 2 == 0:
            return rt
        # Make sure to pick a rounding target
        # that's <= 1 away from x in absolute value.
        if rt <= t:
            return rt + 1
        return rt - 1

    root_twisted = round_to_even((root * 2**32) / modulus) // 2
    return root, root_twisted


def gen_aarch64_root_of_unity_for_block(layer, block, inv=False, scale=False):
    # We are computing a negacyclic NTT; the twiddles needed here is
    # the second half of the twiddles for a cyclic NTT of twice the size.
    # For ease of calculating the roots, layers are numbers 0 through 7
    # in this function.
    log = bitreverse(pow(2, layer) + block, 8)
    if inv is True:
        log = -log
    root = pow(root_of_unity, log, modulus)

    if scale is True:
        # Integrate scaling by 2**(-8) and Montgomery factor 2**32 into twiddle
        root = root * pow(2, 32 - 8, modulus)

    root, root_twisted = prepare_root_for_barrett(root)
    return root, root_twisted


def gen_aarch64_fwd_ntt_zetas_layer123456():
    # Layers 1,2,3 are merged
    yield from gen_aarch64_root_of_unity_for_block(0, 0)
    yield from gen_aarch64_root_of_unity_for_block(1, 0)
    yield from gen_aarch64_root_of_unity_for_block(1, 1)
    yield from gen_aarch64_root_of_unity_for_block(2, 0)
    yield from gen_aarch64_root_of_unity_for_block(2, 1)
    yield from gen_aarch64_root_of_unity_for_block(2, 2)
    yield from gen_aarch64_root_of_unity_for_block(2, 3)
    yield from (0, 0)  # Padding

    # Layers 4,5,6 are merged
    for block in range(8):  # There are 8 blocks in Layer 4
        yield from gen_aarch64_root_of_unity_for_block(3, block)
        yield from gen_aarch64_root_of_unity_for_block(4, 2 * block + 0)
        yield from gen_aarch64_root_of_unity_for_block(4, 2 * block + 1)
        yield from gen_aarch64_root_of_unity_for_block(5, 4 * block + 0)
        yield from gen_aarch64_root_of_unity_for_block(5, 4 * block + 1)
        yield from gen_aarch64_root_of_unity_for_block(5, 4 * block + 2)
        yield from gen_aarch64_root_of_unity_for_block(5, 4 * block + 3)
        yield from (0, 0)  # Padding


def gen_aarch64_fwd_ntt_zetas_layer78():
    # Layers 4,5,6,7,8 are merged, but we emit roots for 4,5,6
    # in separate arrays than those for 7,8
    for block in range(8):

        # Ordering of blocks is adjusted to suit the transposed internal
        # presentation of the data

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 0)[i]
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 1)[i]
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 2)[i]
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 3)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 0)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 2)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 4)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 6)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 1)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 3)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 5)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 7)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 0 + 4)[i]
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 1 + 4)[i]
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 2 + 4)[i]
            yield gen_aarch64_root_of_unity_for_block(6, 8 * block + 3 + 4)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 0 + 8)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 2 + 8)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 4 + 8)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 6 + 8)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 1 + 8)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 3 + 8)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 5 + 8)[i]
            yield gen_aarch64_root_of_unity_for_block(7, 16 * block + 7 + 8)[i]


def gen_aarch64_intt_zetas_layer78():
    for block in range(16):
        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(6, block * 4 + 0, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(6, block * 4 + 1, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(6, block * 4 + 2, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(6, block * 4 + 3, inv=True)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 0, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 2, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 4, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 6, inv=True)[i]

        for i in range(2):
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 1, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 3, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 5, inv=True)[i]
            yield gen_aarch64_root_of_unity_for_block(7, block * 8 + 7, inv=True)[i]


def gen_aarch64_intt_zetas_layer123456():
    for i in range(16):
        yield from gen_aarch64_root_of_unity_for_block(4, i, inv=True)
        yield from gen_aarch64_root_of_unity_for_block(5, i * 2, inv=True)
        yield from gen_aarch64_root_of_unity_for_block(5, i * 2 + 1, inv=True)

    # The last layer has the scaling by 1/256 integrated in the twiddle
    yield from gen_aarch64_root_of_unity_for_block(0, 0, inv=True, scale=True)

    yield from gen_aarch64_root_of_unity_for_block(1, 0, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(1, 1, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 0, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 1, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 2, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 3, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 0, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 1, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 2, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 3, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 4, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 5, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 6, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(3, 7, inv=True)
    yield from (0, 0)  # Padding


def gen_aarch64_zeta_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLD_ARITH_BACKEND_AARCH64)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_aarch64.h"'
        yield ""
        yield "/*"
        yield " * Table of zeta values used in the AArch64 forward NTT"
        yield " * See autogen for details."
        yield " */"
        yield "MLD_ALIGN const int32_t mld_aarch64_ntt_zetas_layer123456[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_fwd_ntt_zetas_layer123456())
        yield "};"
        yield ""
        yield "MLD_ALIGN const int32_t mld_aarch64_ntt_zetas_layer78[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_fwd_ntt_zetas_layer78())
        yield "};"
        yield ""
        yield "MLD_ALIGN const int32_t mld_aarch64_intt_zetas_layer78[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_intt_zetas_layer78())
        yield "};"
        yield ""
        yield "MLD_ALIGN const int32_t mld_aarch64_intt_zetas_layer123456[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_intt_zetas_layer123456())
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLD_EMPTY_CU(aarch64_zetas)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "mldsa/native/aarch64/src/aarch64_zetas.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_aarch64_rej_uniform_eta_table_rows():
    # The index into the lookup table is an 8-bit bitmap, i.e. a number 0..255.
    # Conceptually, the table entry at index i is a vector of 8 16-bit values, of
    # which only the first popcount(i) are set; those are the indices of the set-bits
    # in i. Concretely, we store each 16-bit index as consecutive 8-bit indices.
    def get_set_bits_idxs(i):
        bits = list(map(int, format(i, "08b")))
        bits.reverse()
        return [bit_idx for bit_idx in range(8) if bits[bit_idx] == 1]

    for i in range(256):
        idxs = get_set_bits_idxs(i)
        # Replace each index by two consecutive indices
        idxs = [j for i in idxs for j in [2 * i, 2 * i + 1]]
        # Pad by 255 (invalid index)
        idxs = idxs + [255] * (16 - len(idxs))
        yield idxs


def gen_aarch64_rej_uniform_eta_table(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLD_ARITH_BACKEND_AARCH64) && \\"
        yield "    !defined(MLD_CONFIG_MULTILEVEL_NO_SHARED)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_aarch64.h"'
        yield ""
        yield "/*"
        yield " * Lookup table used by 16-bit rejection sampling (rej_eta)."
        yield " * Adapted from ML-KEM for ML-DSA eta rejection sampling."
        yield " * See autogen for details."
        yield " */"
        yield "MLD_ALIGN const uint8_t mld_rej_uniform_eta_table[] = {"
        for i, idxs in enumerate(gen_aarch64_rej_uniform_eta_table_rows()):
            yield ",".join(map(str, idxs)) + f" /* {i} */,"
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLD_EMPTY_CU(aarch64_rej_uniform_eta_table)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "mldsa/native/aarch64/src/rej_uniform_eta_table.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_aarch64_rej_uniform_table_rows():
    # The index into the lookup table is an 4-bit bitmap, i.e. a number 0..15.
    # Conceptually, the table entry at index i is a vector of 4-bit values, of
    # which only the first popcount(i) are set; those are the indices of the set-bits
    # in i. Concretely, we store each 32-bit index as consecutive 8-bit indices.
    def get_set_bits_idxs(i):
        bits = list(map(int, format(i, "08b")))
        bits.reverse()
        return [bit_idx for bit_idx in range(8) if bits[bit_idx] == 1]

    for i in range(16):
        idxs = get_set_bits_idxs(i)
        # Replace each index by two consecutive indices
        idxs = [j for i in idxs for j in [4 * i + k for k in range(4)]]
        # Pad by -1
        idxs = idxs + [255] * (16 - len(idxs))
        yield idxs


def gen_aarch64_rej_uniform_table(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLD_ARITH_BACKEND_AARCH64) && \\"
        yield "    !defined(MLD_CONFIG_MULTILEVEL_NO_SHARED)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_aarch64.h"'
        yield ""
        yield "/*"
        yield " * Lookup table used by rejection sampling of the public matrix."
        yield " * See autogen for details."
        yield " */"
        yield "MLD_ALIGN const uint8_t mld_rej_uniform_table[] = {"
        for i, idxs in enumerate(gen_aarch64_rej_uniform_table_rows()):
            yield ",".join(map(str, idxs)) + f" /* {i} */,"
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLD_EMPTY_CU(aarch64_rej_uniform_table)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "mldsa/native/aarch64/src/rej_uniform_table.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_avx2_rej_uniform_table_rows():
    # The index into the lookup table is an 8-bit bitmap, i.e. a number 0..255.
    # Conceptually, the table entry at index i is a vector of 8 16-bit values, of
    # which only the first popcount(i) are set; those are the indices of the set-bits
    # in i.
    def get_set_bits_idxs(i):
        bits = list(map(int, format(i, "08b")))
        bits.reverse()
        return [bit_idx for bit_idx in range(8) if bits[bit_idx] == 1]

    for i in range(256):
        idxs = get_set_bits_idxs(i)
        idxs = [i for i in idxs]
        # Pad by 0
        idxs = idxs + [0] * (8 - len(idxs))
        yield "{" + ",".join(map(str, idxs)) + "}"


def gen_avx2_rej_uniform_table(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLD_ARITH_BACKEND_X86_64_DEFAULT) && \\"
        yield "    !defined(MLD_CONFIG_MULTILEVEL_NO_SHARED)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_x86_64.h"'
        yield ""
        yield "/*"
        yield " * Lookup table used by rejection sampling."
        yield " * See autogen for details."
        yield " */"
        yield "MLD_ALIGN const uint8_t mld_rej_uniform_table[256][8] = {"
        yield from map(lambda t: str(t) + ",", gen_avx2_rej_uniform_table_rows())
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLD_EMPTY_CU(avx2_rej_uniform_table)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "mldsa/native/x86_64/src/rej_uniform_table.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def signed_reduce(a):
    """Return signed canonical representative of a mod b"""
    c = a % modulus
    if c >= modulus / 2:
        c -= modulus
    return c


def signed_reduce_u32(a):
    """Return signed canonical representative of a mod b"""
    c = a % 2**32
    if c >= 2**31:
        c -= 2**32
    return c


def prepare_root_for_montmul(root, mult):
    """Takes a constant that the code needs to Montgomery-multiply with,
    and returns the pair of (a) the signed canonical representative of its
    Montgomery form, (b) the twisted constant used in the low-mul part of
    the Montgomery multiplication."""

    # Convert to Montgomery form and pick canonical signed representative
    root = signed_reduce(root * montgomery_factor)
    if mult:
        root = signed_reduce_u32(root * pow(modulus, -1, 2**32))
    return root


def gen_avx2_root_of_unity_for_block(layer, block, mult=False):
    # We are computing a negacyclic NTT; the twiddles needed here is
    # the second half of the twiddles for a cyclic NTT of twice the size.
    log = bitreverse(pow(2, layer) + block, 8)
    root = pow(root_of_unity, log, modulus)
    return prepare_root_for_montmul(root, mult)


def gen_avx2_fwd_ntt_zetas(mult=False):

    def gen_twiddles(layer, block, repeat, mult):
        root = gen_avx2_root_of_unity_for_block(layer, block, mult)
        return [root] * repeat

    def gen_twiddles_many(layer, block_base, block_offsets, repeat, mult):
        roots = list(
            map(
                lambda x: gen_twiddles(layer, block_base + x, repeat, mult),
                block_offsets,
            )
        )
        yield from (r for l in roots for r in l)

    # embed the scaling of 1/256 and correction of the Montgomery factor
    # from the basemul into last twiddle of the inverse NTT
    # - root^-128 * 2^64/256
    # In the forward NTT this twiddle is unused
    f = signed_reduce(-pow(root_of_unity, -128, modulus) * 2**56)
    if mult:
        f = signed_reduce_u32(f * pow(modulus, -1, 2**32))

    yield f

    # Layers 1 twiddle
    # In the inverse NTT this twiddle is unused
    yield from gen_twiddles_many(0, 0, range(1), 1, mult)

    # Layer 2-8 twiddles
    yield from gen_twiddles_many(1, 0, range(2), 1, mult)
    yield from gen_twiddles_many(2, 0, range(4), 1, mult)
    yield from gen_twiddles_many(3, 0, range(8), 4, mult)
    yield from gen_twiddles_many(4, 0, range(16), 2, mult)
    yield from gen_twiddles_many(5, 0, range(32), 1, mult)
    for i in range(32):
        yield from gen_twiddles_many(6, i * 2, range(1), 1, mult)
    for i in range(32):
        yield from gen_twiddles_many(6, i * 2 + 1, range(1), 1, mult)

    for k in range(4):
        for i in range(32):
            yield from gen_twiddles_many(7, i * 4 + k, range(1), 1, mult)


def gen_avx2_zeta_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield "/*"
        yield " * Table of zeta values used in the AVX2 NTTs"
        yield " * See autogen for details."
        yield " */"
        yield "/* twiddles * q^-1 */"
        yield from map(lambda t: str(t) + ",", gen_avx2_fwd_ntt_zetas(mult=True))
        yield "/* twiddles */"
        yield ""
        yield from map(lambda t: str(t) + ",", gen_avx2_fwd_ntt_zetas(mult=False))
        yield ""

    update_file(
        "mldsa/native/x86_64/src/x86_64_zetas.i", "\n".join(gen()), dry_run=dry_run
    )


def adjust_header_guard_for_filename(content, header_file):

    status_update("header guards", header_file)

    content = content.split("\n")
    exceptions = {}

    # Use full filename as the header guard, with '/' and '.' replaced by '_'
    guard_name = (
        header_file.removeprefix("mldsa/").replace("/", "_").replace(".", "_").upper()
    )
    guard_name = "MLD_" + guard_name

    if header_file in exceptions.keys():
        guard_name = exceptions[header_file]

    def gen_copyright(include_mlkem=False):
        yield "/*"
        if include_mlkem is True:
            yield " * Copyright (c) The mlkem-native project authors"
        yield " * Copyright (c) The mldsa-native project authors"
        yield " * SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT"
        yield " */"

    def gen_guard():
        yield f"#ifndef {guard_name}"
        yield f"#define {guard_name}"

    def gen_footer():
        yield f"#endif"
        yield ""

    cr = list(gen_copyright())
    cr_alt = list(gen_copyright(include_mlkem=True))
    guard = list(gen_guard())
    footer = list(gen_footer())

    # Check if header file begins with copyright notice
    if content[: len(cr)] == cr:
        i = len(cr)
    elif content[: len(cr_alt)] == cr_alt:
        i = len(cr_alt)
    else:
        assert False

    while content[i].strip() == "":
        i += 1
    # Check if header file has some guard -- if so, drop it
    if content[i].strip().startswith("#if !defined") or content[i].strip().startswith(
        "#ifndef"
    ):
        del content[i]
        if content[i].strip().startswith("#define"):
            del content[i]
        has_guard = True
    else:
        has_guard = False
    # Add standardized guard
    content = content[:i] + guard + content[i:]
    # Check if header has some footer
    if (
        has_guard is True
        and content[-1] == ""
        and content[-2].strip().startswith("#endif")
    ):
        del content[-2:]
    # Add standardized footer
    content = content + footer

    return "\n".join(content)


def gen_header_guard(header_file, dry_run=False):
    with open(header_file, "r") as f:
        content = f.read()
    new_content = adjust_header_guard_for_filename(content, header_file)
    update_file(header_file, new_content, dry_run=dry_run)


def gen_header_guards(dry_run=False):
    with ThreadPoolExecutor() as executor:
        _ = list(
            executor.map(partial(gen_header_guard, dry_run=dry_run), get_header_files())
        )


def _main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--dry-run", default=False, action="store_true")

    args = parser.parse_args()

    os.chdir(os.path.join(os.path.dirname(__file__), ".."))

    gen_c_zeta_file(args.dry_run)
    gen_aarch64_zeta_file(args.dry_run)
    gen_aarch64_rej_uniform_table(args.dry_run)
    gen_aarch64_rej_uniform_eta_table(args.dry_run)
    gen_avx2_zeta_file(args.dry_run)
    gen_avx2_rej_uniform_table(args.dry_run)
    gen_header_guards(args.dry_run)
    gen_preprocessor_comments(args.dry_run)

    print()


if __name__ == "__main__":
    _main()
